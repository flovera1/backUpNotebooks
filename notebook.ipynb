{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_from_tweet = 1200000\n",
    "_packet_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/learnsity/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/learnsity/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytypo\n",
      "Installing collected packages: pytypo\n",
      "Successfully installed pytypo-0.3.0\n",
      "Collecting pyspellchecker\n",
      "  Using cached https://files.pythonhosted.org/packages/3b/62/e9da86d71e3ccc500b979f0afb88c1f3ae151766004a0de92775b686a311/pyspellchecker-0.5.2-py2.py3-none-any.whl\n",
      "Installing collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.5.2\n",
      "Collecting bs4\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Using cached https://files.pythonhosted.org/packages/f9/d9/183705a87492249b212d88eef740995f55076195bcf45ed59306c146e42d/beautifulsoup4-4.8.1-py2-none-any.whl\n",
      "Collecting soupsieve>=1.2 (from beautifulsoup4->bs4)\n",
      "  Using cached https://files.pythonhosted.org/packages/5d/42/d821581cf568e9b7dfc5b415aa61952b0f5e3dede4f3cbd650e3a1082992/soupsieve-1.9.4-py2.py3-none-any.whl\n",
      "Collecting backports.functools-lru-cache; python_version < \"3\" (from soupsieve>=1.2->beautifulsoup4->bs4)\n",
      "  Using cached https://files.pythonhosted.org/packages/03/8e/2424c0e65c4a066e28f539364deee49b6451f8fcd4f718fefa50cc3dcf48/backports.functools_lru_cache-1.5-py2.py3-none-any.whl\n",
      "Installing collected packages: backports.functools-lru-cache, soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed backports.functools-lru-cache-1.5 beautifulsoup4-4.8.1 bs4-0.0.1 soupsieve-1.9.4\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "!pip install pytypo\n",
    "!pip install pyspellchecker\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                                               text\n",
       "0          0  1467810369  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1          0  1467810672  is upset that he can't update his Facebook by ...\n",
       "2          0  1467810917  @Kenichan I dived many times for the ball. Man...\n",
       "3          0  1467811184    my whole body feels itchy and like its on fire \n",
       "4          0  1467811193  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "_cols = ['sentiment','id','date','query_string','user','text']\n",
    "_df = pd.read_csv('data.csv',header=None, names=_cols, low_memory=False)\n",
    "_df = _df.drop(columns=['date', 'query_string','user'])\n",
    "_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from bs4 import BeautifulSoup\n",
    "from spellchecker import SpellChecker\n",
    "import string\n",
    "import re\n",
    "import pytypo\n",
    "import json\n",
    "\n",
    "# Tokenizers\n",
    "_tweetok = TweetTokenizer()\n",
    "_wptok = WordPunctTokenizer()\n",
    "\n",
    "# Contractions Dictionary\n",
    "_contraction_dict = json.load(open('contraction_dict.json', 'r'))\n",
    "\n",
    "# Stop Words\n",
    "_stop_words = set(stopwords.words('english'))\n",
    "_exclude = [\"not\", \"no\", \"should\"]\n",
    "_stop_words = [w for w in _stop_words if not w in _exclude]\n",
    "\n",
    "# Regular Expressions\n",
    "_r1 = r'@[A-Za-z0-9_]+' # Twitter username\n",
    "_r2 = r'http\\S+|www\\S+|[A-Za-z0-9\\/.:]*\\.com(\\.[A-Za-z]+)*' # URLs\n",
    "_r3 = r'#[A-Za-z0-9]+' # HashTags\n",
    "_final_regex = r'|'.join((_r1, _r2, _r3)) # Final RegEx\n",
    "\n",
    "# Spell Checker\n",
    "_spell = SpellChecker()\n",
    "_spell.word_frequency.load_words(['microsoft', 'apple', 'google','twitter', 'facebook', 'amazon', 'texting', 'nah'])\n",
    "\n",
    "# Lemmatization\n",
    "_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(valTweet):\n",
    "    \n",
    "    # HTML decoding\n",
    "    valTweet = BeautifulSoup(valTweet, 'lxml').get_text()\n",
    "    \n",
    "    # Weird characters\n",
    "    valTweet = ''.join(filter(lambda x: x in string.printable, valTweet))\n",
    "    \n",
    "    # URLs, mentions and hashtags\n",
    "    valTweet = re.sub(_final_regex, '', valTweet)\n",
    "    \n",
    "    # To lowercase\n",
    "    valTweet = valTweet.lower()\n",
    "    \n",
    "    # Expand contractions\n",
    "    vTokens = _tweetok.tokenize(valTweet)\n",
    "    valTweet = ' '.join([_contraction_dict[w] if w in _contraction_dict else w for w in vTokens]).strip()\n",
    "    \n",
    "    # Only alpha characters\n",
    "    valTweet = re.sub(r'[^a-zA-Z]', ' ', valTweet)\n",
    "    \n",
    "    # Typos correction\n",
    "    vTokens = _wptok.tokenize(valTweet)\n",
    "    vTokens = [pytypo.correct(w) for w in vTokens]\n",
    "    \n",
    "    # Lonely letters removal\n",
    "    vTokens = [w for w in vTokens if len(w) > 1]\n",
    "    \n",
    "    return vTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spelling(valTokens):\n",
    "    \n",
    "    # Discard null tweet\n",
    "    if(len(valTokens) == 0):\n",
    "        return \"\"\n",
    "    \n",
    "    # Long word check, discard tweet if there is a long word\n",
    "    if(len([w for w in valTokens if len(w) > 40]) > 0):\n",
    "        return \"\"\n",
    "    \n",
    "    # Misspelled words correction\n",
    "    vMisspelled = _spell.unknown(valTokens)\n",
    "    if(len(vMisspelled) > 0):\n",
    "        valTokens = [w if not w in vMisspelled else _spell.correction(w) for w in valTokens]\n",
    "        # Stopword drop\n",
    "        \n",
    "    valTokens = [w for w in valTokens if not w in _stop_words]\n",
    "        \n",
    "    return valTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(valTokens):\n",
    "    return [_lemmatizer.lemmatize(w) for w in valTokens] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeated(valTokens):\n",
    "    return ' '.join(sorted(set(valTokens), key=valTokens.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the hills  last one D:\n",
      "teamviewer is entertaining \n",
      "i intensely miss my bestfriends! much love!!  [two more days]\n",
      "Watching the Hills season finale!!! Can't wait for the world premier of the New Moon clip!!!! \n",
      "The Hills and MTV music awards  what a way to end my weekend!\n",
      "@KempEquine thanks  It was a lot of fun!\n",
      "@JerZDre nothin much doin' the same--&gt;its a Sunday I feel like I'm breaking a rule by doin' anything today besides chill \n",
      "@KempEquine thanks  It was a lot of fun!\n",
      "is lovin the sun \n",
      "just one hour left to the awards  ... watchin' the red carpet right now! @ashleytisdale u look amazing \n"
     ]
    }
   ],
   "source": [
    "_texts = []\n",
    "_sentiments = []\n",
    "_ids = []\n",
    "aux = _from_tweet + _packet_size\n",
    "_to_tweet = aux if aux < len(_df['text']) else len(_df['text'])\n",
    "for i in range(_from_tweet,_to_tweet):\n",
    "    print(_df['text'][i])\n",
    "    vTweet = preprocess_tweet(_df['text'][i])\n",
    "    vTweet = correct_spelling(vTweet)\n",
    "    vTweet = lemmatize(vTweet)\n",
    "    vTweet = remove_repeated(vTweet)\n",
    "    if vTweet != \"\":\n",
    "        _texts.append(vTweet)\n",
    "        _sentiments.append(_df['sentiment'][i])\n",
    "        _ids.append(_df['id'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "_cleaner_df = pd.DataFrame(_texts,columns=['text'])\n",
    "_cleaner_df['sentiment'] = _sentiments\n",
    "_cleaner_df['id'] = _ids\n",
    "_cleaner_df.to_csv('cooked_tweets_' + str(_from_tweet) + '_' + str(_to_tweet) + '.csv', index = None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 3)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
