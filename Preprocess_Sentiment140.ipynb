{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_from_tweet = 1200000\n",
    "_packet_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mgmdi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytypo in /home/mgmdi/anaconda3/lib/python3.7/site-packages (0.3.0)\n",
      "Requirement already satisfied: pyspellchecker in /home/mgmdi/anaconda3/lib/python3.7/site-packages (0.5.2)\n",
      "Requirement already satisfied: bs4 in /home/mgmdi/anaconda3/lib/python3.7/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/mgmdi/anaconda3/lib/python3.7/site-packages (from bs4) (4.6.3)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "!pip install pytypo\n",
    "!pip install pyspellchecker\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "_cols = ['sentiment','id','date','query_string','user','text']\n",
    "_df = pd.read_csv('../input/sentiment140/training.1600000.processed.noemoticon.csv',header=None, names=_cols, encoding = 'ISO-8859-1', low_memory=False)\n",
    "_df = _df.drop(columns=['date', 'query_string','user'])\n",
    "_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '../input/contraction-dict/contraction_dict.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c2844582156f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Contractions Dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0m_contraction_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/contraction-dict/contraction_dict.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Stop Words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '../input/contraction-dict/contraction_dict.json'"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from bs4 import BeautifulSoup\n",
    "from spellchecker import SpellChecker\n",
    "import string\n",
    "import re\n",
    "import pytypo\n",
    "import json\n",
    "\n",
    "# Tokenizers\n",
    "_tweetok = TweetTokenizer()\n",
    "_wptok = WordPunctTokenizer()\n",
    "\n",
    "# Contractions Dictionary\n",
    "_contraction_dict = json.load(open('../input/contraction-dict/contraction_dict.json', 'r'))\n",
    "\n",
    "# Stop Words\n",
    "_stop_words = set(stopwords.words('english'))\n",
    "_exclude = [\"not\", \"no\", \"should\"]\n",
    "_stop_words = [w for w in _stop_words if not w in _exclude]\n",
    "\n",
    "# Regular Expressions\n",
    "_r1 = r'@[A-Za-z0-9_]+' # Twitter username\n",
    "_r2 = r'http\\S+|www\\S+|[A-Za-z0-9\\/.:]*\\.com(\\.[A-Za-z]+)*' # URLs\n",
    "_r3 = r'#[A-Za-z0-9]+' # HashTags\n",
    "_final_regex = r'|'.join((_r1, _r2, _r3)) # Final RegEx\n",
    "\n",
    "# Spell Checker\n",
    "_spell = SpellChecker()\n",
    "_spell.word_frequency.load_words(['microsoft', 'apple', 'google','twitter', 'facebook', 'amazon', 'texting', 'nah'])\n",
    "\n",
    "# Lemmatization\n",
    "_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(valTweet):\n",
    "    \n",
    "    # HTML decoding\n",
    "    valTweet = BeautifulSoup(valTweet, 'lxml').get_text()\n",
    "    \n",
    "    # Weird characters\n",
    "    valTweet = ''.join(filter(lambda x: x in string.printable, valTweet))\n",
    "    \n",
    "    # URLs, mentions and hashtags\n",
    "    valTweet = re.sub(_final_regex, '', valTweet)\n",
    "    \n",
    "    # To lowercase\n",
    "    valTweet = valTweet.lower()\n",
    "    \n",
    "    # Expand contractions\n",
    "    vTokens = _tweetok.tokenize(valTweet)\n",
    "    valTweet = ' '.join([_contraction_dict[w] if w in _contraction_dict else w for w in vTokens]).strip()\n",
    "    \n",
    "    # Only alpha characters\n",
    "    valTweet = re.sub(r'[^a-zA-Z]', ' ', valTweet)\n",
    "    \n",
    "    # Typos correction\n",
    "    vTokens = _wptok.tokenize(valTweet)\n",
    "    vTokens = [pytypo.correct(w) for w in vTokens]\n",
    "    \n",
    "    # Lonely letters removal\n",
    "    vTokens = [w for w in vTokens if len(w) > 1]\n",
    "    \n",
    "    return vTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spelling(valTokens):\n",
    "    \n",
    "    # Discard null tweet\n",
    "    if(len(valTokens) == 0):\n",
    "        return \"\"\n",
    "    \n",
    "    # Long word check, discard tweet if there is a long word\n",
    "    if(len([w for w in valTokens if len(w) > 40]) > 0):\n",
    "        return \"\"\n",
    "    \n",
    "    # Misspelled words correction\n",
    "    vMisspelled = _spell.unknown(valTokens)\n",
    "    if(len(vMisspelled) > 0):\n",
    "        valTokens = [w if not w in vMisspelled else _spell.correction(w) for w in valTokens]\n",
    "        # Stopword drop\n",
    "        \n",
    "    valTokens = [w for w in valTokens if not w in _stop_words]\n",
    "        \n",
    "    return valTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(valTokens):\n",
    "    return [_lemmatizer.lemmatize(w) for w in valTokens] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeated(valTokens):\n",
    "    return ' '.join(sorted(set(valTokens), key=valTokens.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_texts = []\n",
    "_sentiments = []\n",
    "_ids = []\n",
    "aux = _from_tweet + _packet_size\n",
    "_to_tweet = aux if aux < len(_df['text']) else len(_df['text'])\n",
    "for i in range(_from_tweet,_to_tweet):\n",
    "    vTweet = preprocess_tweet(_df['text'][i])\n",
    "    vTweet = correct_spelling(vTweet)\n",
    "    vTweet = lemmatize(vTweet)\n",
    "    vTweet = remove_repeated(vTweet)\n",
    "    if vTweet != \"\":\n",
    "        _texts.append(vTweet)\n",
    "        _sentiments.append(_df['sentiment'][i])\n",
    "        _ids.append(_df['id'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_cleaner_df = pd.DataFrame(_texts,columns=['text'])\n",
    "_cleaner_df['sentiment'] = _sentiments\n",
    "_cleaner_df['id'] = _ids\n",
    "_cleaner_df.to_csv('cooked_tweets_' + str(_from_tweet) + '_' + str(_to_tweet) + '.csv', index = None, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
